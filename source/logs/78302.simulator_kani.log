cpu-bind=MASK - nlpgpu04, task  0  0 [46382]: mask 0x3f0300003f03 set
cpu-bind=MASK - nlpgpu08, task  1  0 [32961]: mask 0xff000000ff set
cpu-bind=MASK - nlpgpu04, task  0  0 [46428]: mask 0x3f0300003f03 set
Downloading shards:   0%|          | 0/8 [00:00<?, ?it/s]Downloading shards:   0%|          | 0/8 [00:13<?, ?it/s]
Downloading shards:   0%|          | 0/8 [00:00<?, ?it/s]Downloading shards:   0%|          | 0/8 [00:24<?, ?it/s]
Downloading shards:   0%|          | 0/8 [00:00<?, ?it/s]Downloading shards:   0%|          | 0/8 [00:39<?, ?it/s]
Error running model: Could not load model google/gemma-2-9b with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>, <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'>). See the original errors:

while loading with AutoModelForCausalLM, an error is thrown:
Traceback (most recent call last):
  File "/home1/a/adilsha/miniconda3/lib/python3.12/site-packages/transformers/pipelines/base.py", line 288, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home1/a/adilsha/miniconda3/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home1/a/adilsha/miniconda3/lib/python3.12/site-packages/transformers/modeling_utils.py", line 3769, in from_pretrained
    resolved_archive_file, sharded_metadata = get_checkpoint_shard_files(
                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home1/a/adilsha/miniconda3/lib/python3.12/site-packages/transformers/utils/hub.py", line 1098, in get_checkpoint_shard_files
    cached_filename = cached_file(
                      ^^^^^^^^^^^^
  File "/home1/a/adilsha/miniconda3/lib/python3.12/site-packages/transformers/utils/hub.py", line 403, in cached_file
    resolved_file = hf_hub_download(
                    ^^^^^^^^^^^^^^^^
  File "/home1/a/adilsha/miniconda3/lib/python3.12/site-packages/huggingface_hub/utils/_deprecation.py", line 101, in inner_f
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/home1/a/adilsha/miniconda3/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home1/a/adilsha/miniconda3/lib/python3.12/site-packages/huggingface_hub/file_download.py", line 1232, in hf_hub_download
    return _hf_hub_download_to_cache_dir(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home1/a/adilsha/miniconda3/lib/python3.12/site-packages/huggingface_hub/file_download.py", line 1381, in _hf_hub_download_to_cache_dir
    _download_to_tmp_and_move(
  File "/home1/a/adilsha/miniconda3/lib/python3.12/site-packages/huggingface_hub/file_download.py", line 1915, in _download_to_tmp_and_move
    http_get(
  File "/home1/a/adilsha/miniconda3/lib/python3.12/site-packages/huggingface_hub/file_download.py", line 544, in http_get
    temp_file.write(chunk)
OSError: [Errno 122] Disk quota exceeded

while loading with Gemma2ForCausalLM, an error is thrown:
Traceback (most recent call last):
  File "/home1/a/adilsha/miniconda3/lib/python3.12/site-packages/transformers/pipelines/base.py", line 288, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home1/a/adilsha/miniconda3/lib/python3.12/site-packages/transformers/modeling_utils.py", line 3769, in from_pretrained
    resolved_archive_file, sharded_metadata = get_checkpoint_shard_files(
                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home1/a/adilsha/miniconda3/lib/python3.12/site-packages/transformers/utils/hub.py", line 1098, in get_checkpoint_shard_files
    cached_filename = cached_file(
                      ^^^^^^^^^^^^
  File "/home1/a/adilsha/miniconda3/lib/python3.12/site-packages/transformers/utils/hub.py", line 403, in cached_file
    resolved_file = hf_hub_download(
                    ^^^^^^^^^^^^^^^^
  File "/home1/a/adilsha/miniconda3/lib/python3.12/site-packages/huggingface_hub/utils/_deprecation.py", line 101, in inner_f
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/home1/a/adilsha/miniconda3/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home1/a/adilsha/miniconda3/lib/python3.12/site-packages/huggingface_hub/file_download.py", line 1232, in hf_hub_download
    return _hf_hub_download_to_cache_dir(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home1/a/adilsha/miniconda3/lib/python3.12/site-packages/huggingface_hub/file_download.py", line 1381, in _hf_hub_download_to_cache_dir
    _download_to_tmp_and_move(
  File "/home1/a/adilsha/miniconda3/lib/python3.12/site-packages/huggingface_hub/file_download.py", line 1903, in _download_to_tmp_and_move
    with incomplete_path.open("ab") as f:
OSError: [Errno 122] Disk quota exceeded



Downloading shards:   0%|          | 0/8 [00:00<?, ?it/s]Downloading shards:   0%|          | 0/8 [00:50<?, ?it/s]
Error running model: Could not load model google/gemma-2-9b with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>, <class 'transformers.models.gemma2.modeling_gemma2.Gemma2ForCausalLM'>). See the original errors:

while loading with AutoModelForCausalLM, an error is thrown:
Traceback (most recent call last):
  File "/home1/a/adilsha/miniconda3/lib/python3.12/site-packages/transformers/pipelines/base.py", line 288, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home1/a/adilsha/miniconda3/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py", line 564, in from_pretrained
    return model_class.from_pretrained(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home1/a/adilsha/miniconda3/lib/python3.12/site-packages/transformers/modeling_utils.py", line 3769, in from_pretrained
    resolved_archive_file, sharded_metadata = get_checkpoint_shard_files(
                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home1/a/adilsha/miniconda3/lib/python3.12/site-packages/transformers/utils/hub.py", line 1098, in get_checkpoint_shard_files
    cached_filename = cached_file(
                      ^^^^^^^^^^^^
  File "/home1/a/adilsha/miniconda3/lib/python3.12/site-packages/transformers/utils/hub.py", line 403, in cached_file
    resolved_file = hf_hub_download(
                    ^^^^^^^^^^^^^^^^
  File "/home1/a/adilsha/miniconda3/lib/python3.12/site-packages/huggingface_hub/utils/_deprecation.py", line 101, in inner_f
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/home1/a/adilsha/miniconda3/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home1/a/adilsha/miniconda3/lib/python3.12/site-packages/huggingface_hub/file_download.py", line 1232, in hf_hub_download
    return _hf_hub_download_to_cache_dir(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home1/a/adilsha/miniconda3/lib/python3.12/site-packages/huggingface_hub/file_download.py", line 1381, in _hf_hub_download_to_cache_dir
    _download_to_tmp_and_move(
  File "/home1/a/adilsha/miniconda3/lib/python3.12/site-packages/huggingface_hub/file_download.py", line 1915, in _download_to_tmp_and_move
    http_get(
  File "/home1/a/adilsha/miniconda3/lib/python3.12/site-packages/huggingface_hub/file_download.py", line 544, in http_get
    temp_file.write(chunk)
OSError: [Errno 122] Disk quota exceeded

while loading with Gemma2ForCausalLM, an error is thrown:
Traceback (most recent call last):
  File "/home1/a/adilsha/miniconda3/lib/python3.12/site-packages/transformers/pipelines/base.py", line 288, in infer_framework_load_model
    model = model_class.from_pretrained(model, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home1/a/adilsha/miniconda3/lib/python3.12/site-packages/transformers/modeling_utils.py", line 3769, in from_pretrained
    resolved_archive_file, sharded_metadata = get_checkpoint_shard_files(
                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home1/a/adilsha/miniconda3/lib/python3.12/site-packages/transformers/utils/hub.py", line 1098, in get_checkpoint_shard_files
    cached_filename = cached_file(
                      ^^^^^^^^^^^^
  File "/home1/a/adilsha/miniconda3/lib/python3.12/site-packages/transformers/utils/hub.py", line 403, in cached_file
    resolved_file = hf_hub_download(
                    ^^^^^^^^^^^^^^^^
  File "/home1/a/adilsha/miniconda3/lib/python3.12/site-packages/huggingface_hub/utils/_deprecation.py", line 101, in inner_f
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/home1/a/adilsha/miniconda3/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py", line 114, in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home1/a/adilsha/miniconda3/lib/python3.12/site-packages/huggingface_hub/file_download.py", line 1232, in hf_hub_download
    return _hf_hub_download_to_cache_dir(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home1/a/adilsha/miniconda3/lib/python3.12/site-packages/huggingface_hub/file_download.py", line 1381, in _hf_hub_download_to_cache_dir
    _download_to_tmp_and_move(
  File "/home1/a/adilsha/miniconda3/lib/python3.12/site-packages/huggingface_hub/file_download.py", line 1915, in _download_to_tmp_and_move
    http_get(
  File "/home1/a/adilsha/miniconda3/lib/python3.12/site-packages/huggingface_hub/file_download.py", line 544, in http_get
    temp_file.write(chunk)
OSError: [Errno 122] Disk quota exceeded



